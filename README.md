# ğŸ¤– RAG Chat - Assistant IA avec Recherche Vectorielle

Un systÃ¨me de chat intelligent qui utilise la technologie RAG (Retrieval-Augmented Generation) pour rÃ©pondre aux questions en se basant sur votre documentation locale.

## ğŸ“‹ FonctionnalitÃ©s

- ğŸ’¬ **Interface de chat moderne** avec React et TailwindCSS
- ğŸ” **Recherche vectorielle** pour trouver les informations pertinentes
- ğŸ“š **Ingestion automatique** de documents (`.txt`, `.md`)
- ğŸ§  **RÃ©ponses contextualisÃ©es** grÃ¢ce Ã  OpenAI GPT-4
- âš¡ **Performance optimale** avec Vite et TypeScript

---

## ğŸ› ï¸ PrÃ©requis

### SystÃ¨me d'exploitation

- **Windows 10/11** ou **Linux** (Ubuntu, Debian, etc.)

### Logiciels requis

#### Option 1 : Bun (RecommandÃ©)

1. **Installer Bun** :

   **Sur Windows :**

   ```powershell
   powershell -c "irm bun.sh/install.ps1 | iex"
   ```

   **Sur Linux :**

   ```bash
   curl -fsSL https://bun.sh/install | bash
   ```

#### Option 2 : Node.js (Alternative)

1. **Installer Node.js** (version 18 ou supÃ©rieure) :
   - TÃ©lÃ©charger depuis [nodejs.org](https://nodejs.org/)
   - Ou utiliser un gestionnaire de versions comme `nvm`

### ClÃ© API OpenAI

- CrÃ©er un compte sur [OpenAI](https://platform.openai.com/)
- GÃ©nÃ©rer une clÃ© API dans les paramÃ¨tres

---

## ğŸš€ Installation

### 1. Cloner le projet

```bash
git clone <url-du-repo>
cd rag-chat
```

### 2. Installer les dÃ©pendances

**Avec Bun (recommandÃ©) :**

```bash
bun install
```

**Avec npm :**

```bash
npm install
```

### 3. Configuration de l'environnement

CrÃ©er un fichier `.env` Ã  la racine du projet :

**Sur Windows :**

```powershell
echo "VITE_OPENAI_API_KEY=votre_clÃ©_api_ici" > .env
```

**Sur Linux :**

```bash
echo "VITE_OPENAI_API_KEY=votre_clÃ©_api_ici" > .env
```

Remplacez `votre_clÃ©_api_ici` par votre vraie clÃ© API OpenAI.

### 4. PrÃ©parer la documentation

Le dossier `docs/` contient dÃ©jÃ  quelques fichiers d'exemple. Vous pouvez :

- Ajouter vos propres fichiers `.txt` ou `.md` dans le dossier `docs/`
- Modifier les fichiers existants selon vos besoins

---

## ğŸ“– Utilisation

### 1. Ingestion des documents

Avant de dÃ©marrer l'application, vous devez traiter vos documents pour crÃ©er la base vectorielle :

**Avec Bun :**

```bash
bun run ingest
```

**Avec npm :**

```bash
npm run ingest
```

Cette commande va :

- Lire tous les fichiers `.txt` et `.md` du dossier `docs/`
- Les dÃ©couper en chunks intelligents
- GÃ©nÃ©rer des embeddings avec OpenAI
- CrÃ©er le fichier `vectorStore.json`

**Exemple de sortie :**

```
ğŸš€ DÃ©but de l'ingestion RAG...

Traitement de faq.md â†’ 8 chunks
Traitement de guide.md â†’ 12 chunks
Traitement de config.txt â†’ 3 chunks

âœ… Ingestion terminÃ©e : 23 chunks enregistrÃ©s dans vectorStore.json
```

### 2. DÃ©marrer l'application

**Avec Bun :**

```bash
bun run dev
```

**Avec npm :**

```bash
npm run dev
```

L'application sera accessible Ã  l'adresse : http://localhost:5173

### 3. Utiliser l'interface

1. Ouvrez votre navigateur sur http://localhost:5173
2. Tapez votre question dans le champ de saisie
3. L'assistant recherchera dans vos documents et rÃ©pondra avec le contexte appropriÃ©

---

## ğŸ”§ Scripts disponibles

| Script            | Description                              |
| ----------------- | ---------------------------------------- |
| `bun run dev`     | Lance le serveur de dÃ©veloppement        |
| `bun run build`   | Compile l'application pour la production |
| `bun run preview` | PrÃ©visualise la version compilÃ©e         |
| `bun run ingest`  | Lance l'ingestion des documents          |
| `bun run lint`    | VÃ©rifie la qualitÃ© du code               |

---

## ğŸ“ Structure du projet

```
rag-chat/
â”œâ”€â”€ docs/                   # ğŸ“š Vos documents source
â”‚   â”œâ”€â”€ faq.md
â”‚   â”œâ”€â”€ guide.md
â”‚   â””â”€â”€ config.txt
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ ingest.ts          # ğŸ”„ Script d'ingestion
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ App.tsx            # ğŸ  Interface principale
â”‚   â”œâ”€â”€ main.tsx
â”‚   â””â”€â”€ assets/
â”œâ”€â”€ public/
â”œâ”€â”€ vectorStore.json       # ğŸ—‚ï¸ Base vectorielle (gÃ©nÃ©rÃ©)
â”œâ”€â”€ .env                   # ğŸ”‘ ClÃ©s API (Ã  crÃ©er)
â”œâ”€â”€ package.json
â””â”€â”€ README.md
```

---

## ğŸ› ï¸ DÃ©pannage

### ProblÃ¨me : "Vector store non trouvÃ©"

**Solution :** ExÃ©cutez d'abord la commande d'ingestion :

```bash
bun run ingest
```

### ProblÃ¨me : "ClÃ© OpenAI manquante"

**Solution :** VÃ©rifiez votre fichier `.env` :

1. Le fichier existe Ã  la racine du projet
2. La variable est bien nommÃ©e `VITE_OPENAI_API_KEY`
3. La clÃ© API est valide

### ProblÃ¨me : Erreur de compilation TypeScript

**Solution :** VÃ©rifiez la version de TypeScript :

```bash
bun --version  # ou npm --version
```

### ProblÃ¨me : Port dÃ©jÃ  utilisÃ©

**Solution :** Le port 5173 est peut-Ãªtre occupÃ©. Vite choisira automatiquement un autre port ou vous pouvez spÃ©cifier :

```bash
bun run dev -- --port 3000
```

---

## ğŸš€ DÃ©ploiement en production

### 1. Compiler l'application

```bash
bun run build
```

### 2. Servir les fichiers statiques

Les fichiers compilÃ©s se trouvent dans le dossier `dist/`. Vous pouvez les servir avec n'importe quel serveur web (Nginx, Apache, etc.).

### 3. Variables d'environnement

En production, configurez `VITE_OPENAI_API_KEY` selon votre environnement de dÃ©ploiement.

---

## ğŸ“„ Licence

Ce projet est sous licence MIT. Voir le fichier `LICENSE` pour plus de dÃ©tails.

---

## ğŸ¤ Contribution

Les contributions sont les bienvenues ! N'hÃ©sitez pas Ã  :

1. Fork le projet
2. CrÃ©er une branche pour votre fonctionnalitÃ©
3. Commiter vos changements
4. Pousser vers la branche
5. Ouvrir une Pull Request

---

## ğŸ“ Support

Si vous rencontrez des problÃ¨mes :

1. VÃ©rifiez la section [DÃ©pannage](#ğŸ› ï¸-dÃ©pannage)
2. Consultez les issues GitHub
3. Ouvrez une nouvelle issue avec les dÃ©tails du problÃ¨me
