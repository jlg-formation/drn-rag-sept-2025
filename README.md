# ğŸ¤– RAG Chat - Assistant IA avec Recherche Vectorielle

Un systÃ¨me de chat intelligent qui utilise la technologie RAG (Retrieval-Augmented Generation) pour rÃ©pondre aux questions en se basant sur votre documentation locale.

## ğŸ“‹ FonctionnalitÃ©s

- ğŸ’¬ **Interface de chat moderne** avec React et TailwindCSS
- ğŸ” **Recherche vectorielle** pour trouver les informations pertinentes
- ğŸ“š **Ingestion automatique** de documents (`.txt`, `.md`)
- ğŸ§  **RÃ©ponses contextualisÃ©es** grÃ¢ce Ã  OpenAI GPT-4 ou **IA locale via LM Studio**
- âš™ï¸ **Configuration flexible** : OpenAI ou modÃ¨les locaux
- âš¡ **Performance optimale** avec Vite et TypeScript

---

## ğŸ› ï¸ PrÃ©requis

### SystÃ¨me d'exploitation

- **Windows 10/11** ou **Linux** (Ubuntu, Debian, etc.)

### Logiciels requis

#### Option 1 : Bun (RecommandÃ©)

1. **Installer Bun** :

   **Sur Windows :**

   ```powershell
   powershell -c "irm bun.sh/install.ps1 | iex"
   ```

   **Sur Linux :**

   ```bash
   curl -fsSL https://bun.sh/install | bash
   ```

#### Option 2 : Node.js (Alternative)

1. **Installer Node.js** (version 18 ou supÃ©rieure) :
   - TÃ©lÃ©charger depuis [nodejs.org](https://nodejs.org/)
   - Ou utiliser un gestionnaire de versions comme `nvm`

### IA et ModÃ¨les

Vous avez **deux options** pour faire fonctionner l'application :

#### Option A : OpenAI (Cloud)

- CrÃ©er un compte sur [OpenAI](https://platform.openai.com/)
- GÃ©nÃ©rer une clÃ© API dans les paramÃ¨tres

#### Option B : LM Studio (Local)

- TÃ©lÃ©charger [LM Studio](https://lmstudio.ai/)
- TÃ©lÃ©charger un modÃ¨le compatible (ex: Llama, Mistral, etc.)
- Lancer le serveur local sur le port 1234

---

## ğŸš€ Installation

### 1. Cloner le projet

```bash
git clone <url-du-repo>
cd rag-chat
```

### 2. Installer les dÃ©pendances

**Avec Bun (recommandÃ©) :**

```bash
bun install
```

**Avec npm :**

```bash
npm install
```

### 3. Configuration de l'environnement

CrÃ©er un fichier `.env` Ã  la racine du projet en vous basant sur `.env.example` :

#### Pour OpenAI (Cloud)

**Sur Windows :**

```powershell
copy .env.example .env
```

**Sur Linux :**

```bash
cp .env.example .env
```

Puis Ã©ditez le fichier `.env` et configurez :

```env
VITE_OPENAI_API_KEY=sk-votre_cle_openai_ici
```

#### Pour LM Studio (Local)

Ã‰ditez le fichier `.env` avec :

```env
VITE_OPENAI_API_KEY=lm-studio
VITE_OPENAI_BASE_URL=http://localhost:1234/v1
VITE_EMBEDDING_MODEL=nomic-embed-text  # ou un autre modÃ¨le d'embedding
VITE_CHAT_MODEL=llama-3.1-8b-instruct  # ou votre modÃ¨le prÃ©fÃ©rÃ©
```

> **Note :** Vous pouvez aussi configurer ces paramÃ¨tres directement dans l'interface web via le bouton âš™ï¸

### 4. PrÃ©parer la documentation

Le dossier `docs/` contient dÃ©jÃ  quelques fichiers d'exemple. Vous pouvez :

- Ajouter vos propres fichiers `.txt` ou `.md` dans le dossier `docs/`
- Modifier les fichiers existants selon vos besoins

---

## ğŸ“– Utilisation

### 1. Ingestion des documents

Avant de dÃ©marrer l'application, vous devez traiter vos documents pour crÃ©er la base vectorielle.

> **Important pour LM Studio :** Assurez-vous que LM Studio est lancÃ© avec un modÃ¨le d'embedding chargÃ© avant de lancer l'ingestion.

**Avec Bun :**

```bash
bun run ingest
```

**Avec npm :**

```bash
npm run ingest
```

Cette commande va :

- Lire tous les fichiers `.txt` et `.md` du dossier `docs/`
- Les dÃ©couper en chunks intelligents
- GÃ©nÃ©rer des embeddings avec OpenAI ou LM Studio
- CrÃ©er le fichier `vectorStore.json`

**Exemple de sortie :**

```
ğŸš€ DÃ©but de l'ingestion RAG...

Configuration IA :
- API Key: lm-studio...
- Base URL: http://localhost:1234/v1
- ModÃ¨le d'embedding: nomic-embed-text

Traitement de faq.md â†’ 8 chunks
Traitement de guide.md â†’ 12 chunks
Traitement de config.txt â†’ 3 chunks

âœ… Ingestion terminÃ©e : 23 chunks enregistrÃ©s dans vectorStore.json
```

### 2. DÃ©marrer l'application

**Avec Bun :**

```bash
bun run dev
```

**Avec npm :**

```bash
npm run dev
```

L'application sera accessible Ã  l'adresse : http://localhost:5173

### 3. Utiliser l'interface

1. Ouvrez votre navigateur sur http://localhost:5173
2. **Configurez l'IA** (optionnel) :
   - Cliquez sur le bouton âš™ï¸ en haut Ã  droite
   - Configurez l'URL de base pour LM Studio : `http://localhost:1234/v1`
   - Configurez la clÃ© API : `lm-studio` pour LM Studio ou votre clÃ© OpenAI
   - Configurez les modÃ¨les selon votre setup
   - Cliquez sur "Sauvegarder"
3. Tapez votre question dans le champ de saisie
4. L'assistant recherchera dans vos documents et rÃ©pondra avec le contexte appropriÃ©

**Configuration recommandÃ©e pour LM Studio :**

- URL de base : `http://localhost:1234/v1`
- ClÃ© API : `lm-studio`
- ModÃ¨le d'embedding : `nomic-embed-text` (ou celui chargÃ© dans LM Studio)
- ModÃ¨le de chat : `llama-3.1-8b-instruct` (ou votre modÃ¨le prÃ©fÃ©rÃ©)

---

## ğŸ”§ Scripts disponibles

| Script            | Description                              |
| ----------------- | ---------------------------------------- |
| `bun run dev`     | Lance le serveur de dÃ©veloppement        |
| `bun run build`   | Compile l'application pour la production |
| `bun run preview` | PrÃ©visualise la version compilÃ©e         |
| `bun run ingest`  | Lance l'ingestion des documents          |
| `bun run lint`    | VÃ©rifie la qualitÃ© du code               |

---

## ğŸ“ Structure du projet

```
rag-chat/
â”œâ”€â”€ docs/                   # ğŸ“š Vos documents source
â”‚   â”œâ”€â”€ faq.md
â”‚   â”œâ”€â”€ guide.md
â”‚   â””â”€â”€ config.txt
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ ingest.ts          # ğŸ”„ Script d'ingestion
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ App.tsx            # ğŸ  Interface principale
â”‚   â”œâ”€â”€ main.tsx
â”‚   â””â”€â”€ assets/
â”œâ”€â”€ public/
â”œâ”€â”€ vectorStore.json       # ğŸ—‚ï¸ Base vectorielle (gÃ©nÃ©rÃ©)
â”œâ”€â”€ .env                   # ğŸ”‘ ClÃ©s API et configuration
â”œâ”€â”€ .env.example           # ğŸ“‹ Exemple de configuration
â”œâ”€â”€ package.json
â””â”€â”€ README.md
```

---

## ğŸ› ï¸ DÃ©pannage

### ProblÃ¨me : "Vector store non trouvÃ©"

**Solution :** ExÃ©cutez d'abord la commande d'ingestion :

```bash
bun run ingest
```

### ProblÃ¨me : "ClÃ© API manquante"

**Solution :** VÃ©rifiez votre fichier `.env` :

1. Le fichier existe Ã  la racine du projet
2. La variable est bien nommÃ©e `VITE_OPENAI_API_KEY`
3. La clÃ© API est valide

**Pour LM Studio :** La clÃ© peut Ãªtre `lm-studio` et vous devez aussi configurer `VITE_OPENAI_BASE_URL=http://localhost:1234/v1`

### ProblÃ¨me : "Erreur de connexion avec LM Studio"

**Solution :**

1. VÃ©rifiez que LM Studio est bien lancÃ© sur le port 1234
2. VÃ©rifiez qu'un modÃ¨le est chargÃ© dans LM Studio
3. Testez l'endpoint : `curl http://localhost:1234/v1/models`
4. VÃ©rifiez que l'URL de base est bien configurÃ©e : `http://localhost:1234/v1`

### ProblÃ¨me : Erreur de compilation TypeScript

**Solution :** VÃ©rifiez la version de TypeScript :

```bash
bun --version  # ou npm --version
```

### ProblÃ¨me : Port dÃ©jÃ  utilisÃ©

**Solution :** Le port 5173 est peut-Ãªtre occupÃ©. Vite choisira automatiquement un autre port ou vous pouvez spÃ©cifier :

```bash
bun run dev -- --port 3000
```

---

## ğŸ–¥ï¸ Configuration LM Studio (IA Locale)

### Installation et configuration de LM Studio

1. **TÃ©lÃ©charger LM Studio** : [https://lmstudio.ai/](https://lmstudio.ai/)

2. **TÃ©lÃ©charger des modÃ¨les** :

   - **Pour le chat** : Llama 3.1 8B, Mistral 7B, ou tout autre modÃ¨le compatible
   - **Pour les embeddings** : nomic-embed-text ou sentence-transformers

3. **Lancer le serveur** :

   - Ouvrez LM Studio
   - Allez dans l'onglet "Local Server"
   - Chargez votre modÃ¨le de chat
   - Cliquez sur "Start Server" (par dÃ©faut sur le port 1234)

4. **Configuration dans l'app** :
   ```env
   VITE_OPENAI_API_KEY=lm-studio
   VITE_OPENAI_BASE_URL=http://localhost:1234/v1
   VITE_CHAT_MODEL=votre-modele-charge
   VITE_EMBEDDING_MODEL=nomic-embed-text
   ```

### Avantages de LM Studio

- âœ… **ConfidentialitÃ©** : Vos donnÃ©es restent locales
- âœ… **Pas de coÃ»ts** : Aucun frais d'API
- âœ… **Offline** : Fonctionne sans connexion internet
- âœ… **Personnalisation** : Choix libre des modÃ¨les

### ModÃ¨les recommandÃ©s

| Type      | ModÃ¨le                | Taille | Performance       |
| --------- | --------------------- | ------ | ----------------- |
| Chat      | Llama 3.1 8B Instruct | 8B     | Excellente        |
| Chat      | Mistral 7B Instruct   | 7B     | TrÃ¨s bonne        |
| Chat      | Phi-3 Mini            | 3.8B   | Bonne (rapide)    |
| Embedding | nomic-embed-text      | 137M   | Optimale pour RAG |

---

## ğŸš€ DÃ©ploiement en production

### 1. Compiler l'application

```bash
bun run build
```

### 2. Servir les fichiers statiques

Les fichiers compilÃ©s se trouvent dans le dossier `dist/`. Vous pouvez les servir avec n'importe quel serveur web (Nginx, Apache, etc.).

### 3. Variables d'environnement

En production, configurez les variables selon votre environnement de dÃ©ploiement :

**Pour OpenAI :**

- `VITE_OPENAI_API_KEY` : votre clÃ© API OpenAI

**Pour LM Studio ou API compatible :**

- `VITE_OPENAI_API_KEY` : clÃ© d'authentification (peut Ãªtre arbitraire)
- `VITE_OPENAI_BASE_URL` : URL de votre endpoint
- `VITE_EMBEDDING_MODEL` : nom du modÃ¨le d'embedding
- `VITE_CHAT_MODEL` : nom du modÃ¨le de chat

---

## ğŸ“„ Licence

Ce projet est sous licence MIT. Voir le fichier `LICENSE` pour plus de dÃ©tails.

---

## ğŸ¤ Contribution

Les contributions sont les bienvenues ! N'hÃ©sitez pas Ã  :

1. Fork le projet
2. CrÃ©er une branche pour votre fonctionnalitÃ©
3. Commiter vos changements
4. Pousser vers la branche
5. Ouvrir une Pull Request

---

## ğŸ“ Support

Si vous rencontrez des problÃ¨mes :

1. VÃ©rifiez la section [DÃ©pannage](#ğŸ› ï¸-dÃ©pannage)
2. Consultez les issues GitHub
3. Ouvrez une nouvelle issue avec les dÃ©tails du problÃ¨me
